# Why is Fisher-Yates the best shuffle?

##### Author: Tianyi Ge (gty@umich.edu)

Shuffle algorithm is more common than you imagine in our daily life. Besides card shuffling, there are a lot of scenarios requiring shuffle algorithm to guarantee fairness. For instance, it's critical to shuffle the training set in the realm of machine learning to prevent over-fitting.

However, how could we evaluate the performance of a shuffle algorithm? Is it truly fair regarding randomness? Is it efficient to execute when the scale increases?

## Fisher and Yates' algorithm

> The Fisher–Yates shuffle, in its original form, was described in 1938 by [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) and [Frank Yates](https://en.wikipedia.org/wiki/Frank_Yates) in their book *Statistical tables for biological, agricultural and medical research*.
>
> Pseudo code:
>
> ```pseudocode
> To shuffle an array a of n elements (indices 0..n-1):
> 
> for i from n−1 downto 1 do
>   j := random integer such that 0 ≤ j ≤ i
>   exchange a[j] and a[i]
> ```

The procedure of Fisher-Yates shuffle is visualized below left.

---

[wildframe]

---

Each step in this visualization is a swap. Due to the design of Fisher-Yates, an entire shuffle always requires strictly n swaps. But what's the calculations done on the right hand side?

## Shannon Entropy distribution

In the last visualization, the right hand side is showing a synchronous calculation of ***Shannon Entropy***. Shannon Entropy of a shuffled sequence (of size $n$) is calculated as:

1. Calculate the modulo differences between adjacent elements: $$\Delta A_i=A_i-A_{i-1}$$

2. With $n$ differences from above, count how many times each difference $j=\Delta A_i$ occurs, denoted as $C_j$ (the blue numbers above, corresponding to the blue histogram).
3. Calculate the normalized difference $p_j=C_j/n$ (The orange histogram above).
4. Shannon Entropy = $\sum_j^{j\in C} -p_j\log_2(p_j)$

How can this magic metric help us tell if Fisher-Yates is good or bad?

Well, let's introduce some comparisons. Say we apply a sort function on, whose compare function returns random comparison result (in other words, can only make the array messy). We call it ***Randomsort***. Let's see their **Shannon Entropy Distributions** when array size = 50.

##### So far [Count] samples:

---

[wildframe]

---

Given that each shuffled permutation has equal probability to appear, Shannon entropy distribution should be a normal distribution. However, we can clearly see that Randomsort has an **skewed asymmetric** shape. On the other hand, Fisher-Yates' is approaching a normal distribution with more sampling. Thus, Fisher-Yates' gives more **fair** shuffled results.

## Where are the elements going?

Now let's take a look at where the elements go after shuffling. The visualization below is the distribution of original position vs. shuffled position. At first, there's no obvious distinction.

---

[wildframe]

---

If you drag the slider to see tests of larger scale, the Fisher-Yates has a uniform distribution. Each position has almost equal probability to end at. However, Randomsort has apparent patterns on the diagonal, which means that elements are more likely to stay on their original position, which is not a satisfying randomness performance.



##### Source code

 	https://github.com/TimothyGe/shuffle-vis

## Reference

[1] https://medium.com/@oldwestaction/randomness-is-hard-e085decbcbb2

[2] https://combine.se/shuffling-a-card-deck/